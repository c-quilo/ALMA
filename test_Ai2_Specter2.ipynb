{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 19784.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"allenai/specter2_base\")\n",
    "adapter_name = model.load_adapter(\"allenai/specter2\", source=\"hf\", set_active=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 41221.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "#load base model\n",
    "model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "#load the adapter(s) as per the required task, provide an identifier for the adapter in load_as argument and activate it\n",
    "model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"specter2\", set_active=True)\n",
    "\n",
    "papers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "          {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\n",
    "papers = \n",
    "# concatenate title and abstract\n",
    "text_batch = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\n",
    "# preprocess the input\n",
    "inputs = tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                   return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "output = model(**inputs)\n",
    "# take the first token in the batch as the embedding\n",
    "embeddings = output.last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 21050.46it/s]\n",
      "/opt/miniconda3/envs/ALMA/lib/python3.10/site-packages/adapters/loading.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_file, map_location=\"cpu\")\n",
      "Generating Embeddings: 100%|██████████| 57231/57231 [1:03:11<00:00, 15.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 57231 papers and saved embeddings.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the two CSV files\n",
    "gold_standard_csv = \"./dummyPapers/goldStandardPapersList.csv\"\n",
    "all_papers_csv = \"./dummyPapers/testCatalysisPapersList.csv\"\n",
    "\n",
    "gold_df = pd.read_csv(gold_standard_csv)\n",
    "all_df = pd.read_csv(all_papers_csv)\n",
    "\n",
    "# Mark gold vs. non-gold\n",
    "gold_df[\"is_gold\"] = 1\n",
    "all_df[\"is_gold\"] = 0\n",
    " \n",
    "# Combine\n",
    "combined_df = pd.concat([gold_df, all_df], ignore_index=True)\n",
    "\n",
    "combined_df.dropna(subset=[\"abstract\"], inplace=True)  # Drop rows without an abstract\n",
    "combined_df = combined_df[combined_df[\"title\"].notna() | combined_df[\"abstract\"].notna()]\n",
    "\n",
    "# Concatenate title and abstract (if title is missing, use only the abstract)\n",
    "combined_df[\"text\"] = combined_df.apply(\n",
    "    lambda row: (row[\"title\"] + \" [SEP] \" if pd.notna(row[\"title\"]) else \"\") + row[\"abstract\"], axis=1\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter2_base\")\n",
    "model = AutoAdapterModel.from_pretrained(\"allenai/specter2_base\")\n",
    "\n",
    "# Load and activate the Specter2 adapter\n",
    "model.load_adapter(\"allenai/specter2_classification\", source=\"hf\", load_as=\"classification\", set_active=True)\n",
    "\n",
    "# Process the input texts in batches\n",
    "batch_size = 1\n",
    "texts = combined_df[\"text\"].tolist()\n",
    "embeddings = []\n",
    "\n",
    "# Process the input texts in batches with a progress bar\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\", unit=\"batch\"):\n",
    "    # Prepare the batch\n",
    "    batch = texts[i:i+batch_size]\n",
    "    inputs = tokenizer(batch, padding=True, truncation=True,\n",
    "                       return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "\n",
    "    # Compute embeddings\n",
    "    outputs = model(**inputs)\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()  # Take [CLS] token embeddings\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "# Combine embeddings into a single array\n",
    "import numpy as np\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Save embeddings to a file for future use\n",
    "np.save(\"specter2_embeddings.npy\", embeddings)\n",
    "\n",
    "# Save the corresponding paper metadata (e.g., OpenAlex ID) to a CSV\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df[\"embedding_index\"] = np.arange(len(combined_df))  # Add an index for cross-referencing embeddings\n",
    "combined_df.to_csv(\"paper_metadata_with_embeddings.csv\", index=False)\n",
    "\n",
    "print(f\"Processed {len(combined_df)} papers and saved embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ALMA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/miniconda3/envs/ALMA/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/ALMA/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
      "BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "# Load the saved metadata and embeddings\n",
    "metadata_path = \"paper_metadata_with_embeddings.csv\"\n",
    "embeddings_path = \"specter2_embeddings.npy\"\n",
    "\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. UMAP for Dimensionality Reduction\n",
    "# --------------------------------------------------\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_neighbors=15,  # Number of neighbors to consider for manifold learning\n",
    "    min_dist=0.1,    # Minimum distance between points\n",
    "    n_components=2,  # Reduce to 2 dimensions for visualization\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "# Add UMAP results to the metadata DataFrame\n",
    "metadata_df[\"x\"] = embeddings_2d[:, 0]\n",
    "metadata_df[\"y\"] = embeddings_2d[:, 1]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Prepare Bokeh Data Sources\n",
    "# --------------------------------------------------\n",
    "# Create separate data sources for gold (red) and non-gold (blue)\n",
    "red_source = ColumnDataSource(metadata_df[metadata_df[\"is_gold\"] == 1])\n",
    "blue_source = ColumnDataSource(metadata_df[metadata_df[\"is_gold\"] == 0])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Create Bokeh Plot\n",
    "# --------------------------------------------------\n",
    "p = figure(\n",
    "    title=\"UMAP Projection of Specter2 Embeddings\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,save,hover\",\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Plot red points (gold-standard) with alpha=1\n",
    "p.circle(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    source=red_source,\n",
    "    size=6,\n",
    "    fill_alpha=1.0,\n",
    "    color=\"red\",\n",
    "    legend_label=\"Gold (Red)\"\n",
    ")\n",
    "\n",
    "# Plot blue points (non-gold) with alpha=0.5\n",
    "p.circle(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    source=blue_source,\n",
    "    size=6,\n",
    "    fill_alpha=0.5,\n",
    "    color=\"blue\",\n",
    "    legend_label=\"Non-Gold (Blue)\"\n",
    ")\n",
    "\n",
    "# Add hover tool\n",
    "hover_tool = HoverTool()\n",
    "hover_tool.tooltips = [\n",
    "    (\"OAID\", \"@oaid\"),\n",
    "    (\"Title\", \"@title\"),\n",
    "    (\"Category\", \"@color\")\n",
    "]\n",
    "p.add_tools(hover_tool)\n",
    "\n",
    "# Configure legend\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.title = \"Category\"\n",
    "p.legend.click_policy = \"hide\"  # Allows toggling categories\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Show Plot\n",
    "# --------------------------------------------------\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57231, 768)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
